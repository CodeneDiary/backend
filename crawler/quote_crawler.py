# âœ… crawler/quotes_crawler.py
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import requests
from bs4 import BeautifulSoup
import json
import sqlite3
from tqdm import tqdm
from dotenv import load_dotenv
from utils.emotion_utils import infer_emotion_with_model
from utils.db_utils import save_to_db

# âœ… ë²ˆì—­ ê´€ë ¨/
load_dotenv()
GOOGLE_API_KEY = os.getenv("YOUTUBE_API_KEY")
TRANSLATE_URL = "https://translation.googleapis.com/language/translate/v2"

def translate_text(text, target="ko"):
    try:
        params = {
            "q": text,
            "target": target,
            "format": "text",
            "key": GOOGLE_API_KEY
        }
        response = requests.post(TRANSLATE_URL, params=params)
        response.raise_for_status()
        return response.json()["data"]["translations"][0]["translatedText"]
    except Exception as e:
        print(f"[ë²ˆì—­ ì‹¤íŒ¨] {text[:30]}... â†’ {e}")
        return text  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ë°˜í™˜

# âœ… ëª…ì–¸ ì „ì²´ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_all_quotes():
    base_url = "https://quotes.toscrape.com/page/{}/"
    all_quotes = []

    for page in range(1, 11):  # ì´ 10í˜ì´ì§€
        res = requests.get(base_url.format(page))
        soup = BeautifulSoup(res.text, "html.parser")
        quotes = soup.select(".quote")

        if not quotes:
            break

        for quote in quotes:
            text = quote.select_one(".text").get_text(strip=True).strip("â€œâ€")
            author = quote.select_one(".author").get_text(strip=True)

            translated = translate_text(text)
            emotions = infer_emotion_with_model(translated)

            all_quotes.append({
                "text": text,
                "author": author,
                "translated": translated,
                "emotions": emotions
            })

    return all_quotes

# âœ… JSON ì €ì¥
def save_to_json(data, filepath="data/quotes.json"):
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# âœ… ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ“Œ ëª…ì–¸ ì „ì²´ í¬ë¡¤ë§ ì¤‘...")
    quotes = crawl_all_quotes()
    print(f"âœ… ì´ {len(quotes)}ê°œ í¬ë¡¤ë§ ì™„ë£Œ")

    save_to_json(quotes)
   
    # ì „ì²˜ë¦¬: 'translated' í‚¤ë¥¼ 'title'ë¡œ ë°”ê¾¸ê³  urlì€ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¸íŒ…
    quotes_for_db = []
    for item in quotes:
        quotes_for_db.append({
            "title": item["translated"],
            "url": "",
            "emotion_tags": item["emotions"]
        })

    save_to_db("quotes", quotes_for_db)
    print("âœ… JSON ë° DB ì €ì¥ ì™„ë£Œ")




